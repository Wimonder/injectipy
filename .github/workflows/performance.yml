name: Performance

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance benchmarks weekly on Saturdays at 1 AM UTC
    - cron: '0 1 * * 6'

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need full history for comparison

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: 1.7.1
        virtualenvs-create: true
        virtualenvs-in-project: true

    - name: Install dependencies
      run: poetry install --no-interaction

    - name: Run performance benchmarks
      run: |
        poetry run pytest \
          -m performance \
          --benchmark-json=benchmark.json \
          --benchmark-sort=mean \
          --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,ops,rounds \
          --benchmark-warmup=on \
          --benchmark-warmup-iterations=3 \
          -v

    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: 'Injectipy Performance Benchmarks'
        tool: 'pytest'
        output-file-path: benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: ${{ github.ref == 'refs/heads/main' }}
        # Alert if performance degrades by more than 150%
        alert-threshold: '150%'
        comment-on-alert: true
        fail-on-alert: false  # Don't fail CI, just alert
        summary-always: true

    - name: Upload benchmark artifact
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark.json

  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: 1.7.1
        virtualenvs-create: true
        virtualenvs-in-project: true

    - name: Install dependencies and profiling tools
      run: |
        poetry install --no-interaction
        poetry run pip install memory-profiler psutil

    - name: Run memory profiling tests
      run: |
        poetry run python -m pytest \
          tests/test_performance.py::test_memory_usage_with_cached_resolvers \
          -v -s --tb=short

    - name: Profile memory usage of examples
      run: |
        echo "Profiling basic usage example..."
        poetry run python -m memory_profiler examples/basic_usage.py > memory_profile_basic.txt

        echo "Profiling advanced patterns example..."
        poetry run python -m memory_profiler examples/advanced_patterns.py > memory_profile_advanced.txt

        echo "Memory profile results:"
        cat memory_profile_basic.txt
        cat memory_profile_advanced.txt

    - name: Upload memory profiles
      uses: actions/upload-artifact@v3
      with:
        name: memory-profiles
        path: |
          memory_profile_basic.txt
          memory_profile_advanced.txt

  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: 1.7.1
        virtualenvs-create: true
        virtualenvs-in-project: true

    - name: Install dependencies
      run: poetry install --no-interaction

    - name: Create load test script
      run: |
        cat > load_test.py << 'EOF'
        """Load testing script for dependency injection performance."""
        import time
        import threading
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from injectipy import InjectipyStore, Inject, inject

        def setup_complex_dependencies():
            """Set up a complex dependency graph for load testing."""
            store = InjectipyStore()

            # Register 1000 base values
            for i in range(1000):
                store.register_value(f"value_{i}", f"data_{i}")

            # Register 500 resolvers with dependencies
            for i in range(500):
                def make_resolver(index):
                    def resolver(val=Inject[f"value_{index % 1000}"]):
                        return f"resolved_{val}_{index}"
                    return resolver

                resolver_func = make_resolver(i)
                store.register_resolver(f"resolver_{i}", resolver_func)

            return store

        def load_test_concurrent_access():
            """Test concurrent access under high load."""
            store = setup_complex_dependencies()
            results = []
            errors = []

            def worker():
                try:
                    # Access random dependencies
                    import random
                    for _ in range(100):
                        key = f"resolver_{random.randint(0, 499)}"
                        result = store[key]
                        results.append(result)
                except Exception as e:
                    errors.append(str(e))

            start_time = time.time()

            # Run 50 concurrent workers
            with ThreadPoolExecutor(max_workers=50) as executor:
                futures = [executor.submit(worker) for _ in range(50)]
                for future in as_completed(futures):
                    future.result()

            end_time = time.time()

            print(f"Load test completed in {end_time - start_time:.2f}s")
            print(f"Total results: {len(results)}")
            print(f"Total errors: {len(errors)}")

            assert len(errors) == 0, f"Load test failed with errors: {errors[:5]}"
            assert len(results) == 5000, f"Expected 5000 results, got {len(results)}"
            assert end_time - start_time < 30, f"Load test took too long: {end_time - start_time:.2f}s"

            print("‚úÖ Load test passed!")

        if __name__ == "__main__":
            load_test_concurrent_access()
        EOF

    - name: Run load test
      run: poetry run python load_test.py

    - name: Run stress tests
      run: |
        poetry run pytest \
          tests/test_performance.py::test_stress_test_mixed_operations \
          tests/test_performance.py::test_concurrent_resolver_performance \
          tests/test_performance.py::test_singleton_creation_performance \
          -v --tb=short

  performance-report:
    name: Performance Report
    runs-on: ubuntu-latest
    needs: [benchmark, memory-profiling, load-testing]
    if: always()

    steps:
    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results
        path: ./

    - name: Generate performance summary
      run: |
        echo "## üöÄ Performance Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Test Category | Status | Details |" >> $GITHUB_STEP_SUMMARY
        echo "|---------------|--------|---------|" >> $GITHUB_STEP_SUMMARY
        echo "| Benchmarks | ${{ needs.benchmark.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} | Micro-benchmarks and regression detection |" >> $GITHUB_STEP_SUMMARY
        echo "| Memory Profiling | ${{ needs.memory-profiling.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} | Memory usage analysis |" >> $GITHUB_STEP_SUMMARY
        echo "| Load Testing | ${{ needs.load-testing.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} | High concurrency and stress testing |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f benchmark.json ]; then
          echo "### Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Latest benchmark data available in artifacts." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ needs.benchmark.result }}" != "success" ] || \
           [ "${{ needs.memory-profiling.result }}" != "success" ] || \
           [ "${{ needs.load-testing.result }}" != "success" ]; then
          echo "‚ùå One or more performance tests failed"
        else
          echo "‚úÖ All performance tests passed"
        fi
